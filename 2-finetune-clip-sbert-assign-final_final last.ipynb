{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814fe156-7c2b-4ff5-a997-eaa76a695e49",
   "metadata": {},
   "source": [
    " AI/ML Engineer Assignment - Multi-Modal Fashion Recommendation Engine\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "This notebook implements **Task 1** of the fashion recommendation engine assignment:\n",
    "\n",
    "# Objective\n",
    "Train a **multi-modal embedding model** that combines fashion product text + images into a shared semantic embedding space.\n",
    "\n",
    "### Components Covered\n",
    "1. ** Multi-Modal Model Training**: Fine-tune CLIP using contrastive learning\n",
    "2. ** Evaluation Metrics**: Cosine similarity and Top-k accuracy for retrieval  \n",
    "3. ** Model Optimization**: Domain-specific adaptation for fashion terminology\n",
    "4. ** Model Deployment**: Prepare for vector database integration\n",
    "\n",
    "\n",
    "### Dataset\n",
    "- **Source**: Fashion Product Images Dataset (Small) from Kaggle\n",
    "- **Content**: Product images, titles, descriptions, categories, gender, price\n",
    "- **Size**: Optimized subset for efficient training and evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c76683-915f-47ad-9da6-c3dadcc1fda3",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3c51ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting jupyterlab\n",
      "  Downloading jupyterlab-4.4.5-py3-none-any.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "Requirement already satisfied: ipykernel in /venv/main/lib/python3.10/site-packages (6.29.5)\n",
      "Requirement already satisfied: ipywidgets in /venv/main/lib/python3.10/site-packages (8.1.5)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (2.32.3)\n",
      "Collecting isodate\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 KB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting torch==2.3.0\n",
      "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.1-cp310-cp310-manylinux_2_28_x86_64.whl (7.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting transformers==4.48.0\n",
      "  Downloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting sentence-transformers==3.3.1\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.9.0-py3-none-any.whl (367 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/367.1 KB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting jinja2\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.10/site-packages (from torch==2.3.0) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting triton==2.3.0\n",
      "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 KB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /venv/main/lib/python3.10/site-packages (from torch==2.3.0) (4.12.2)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nccl-cu12==2.20.5\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m140.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch==2.3.0) (3.17.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 KB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /venv/main/lib/python3.10/site-packages (from transformers==4.48.0) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /venv/main/lib/python3.10/site-packages (from transformers==4.48.0) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from transformers==4.48.0) (24.2)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from transformers==4.48.0) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.10/site-packages (from transformers==4.48.0) (6.0.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m130.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting Pillow\n",
      "  Using cached pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Collecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: jupyter-core in /venv/main/lib/python3.10/site-packages (from jupyterlab) (5.7.2)\n",
      "Collecting tomli>=1.2.2\n",
      "  Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /venv/main/lib/python3.10/site-packages (from jupyterlab) (59.6.0)\n",
      "Requirement already satisfied: traitlets in /venv/main/lib/python3.10/site-packages (from jupyterlab) (5.14.3)\n",
      "Collecting httpx>=0.25.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting jupyter-server<3,>=2.4.0\n",
      "  Downloading jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 KB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting notebook-shim>=0.2\n",
      "  Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1\n",
      "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting jupyter-lsp>=2.0.0\n",
      "  Downloading jupyter_lsp-2.2.6-py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/69.4 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-lru>=1.0.0\n",
      "  Downloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /venv/main/lib/python3.10/site-packages (from jupyterlab) (6.4.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /venv/main/lib/python3.10/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /venv/main/lib/python3.10/site-packages (from ipykernel) (8.34.0)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.10/site-packages (from ipykernel) (7.0.0)\n",
      "Requirement already satisfied: nest-asyncio in /venv/main/lib/python3.10/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /venv/main/lib/python3.10/site-packages (from ipykernel) (1.8.13)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /venv/main/lib/python3.10/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: pyzmq>=24 in /venv/main/lib/python3.10/site-packages (from ipykernel) (26.2.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /venv/main/lib/python3.10/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /venv/main/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /venv/main/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests) (2.3.0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 KB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m132.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m139.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "  Downloading torchvision-0.20.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "  Downloading torchvision-0.19.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m141.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m132.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading torchaudio-2.6.0-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "  Downloading torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "  Downloading torchaudio-2.5.0-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m130.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  Downloading torchaudio-2.4.1-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  Downloading torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  Downloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting anyio\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Collecting h11>=0.16\n",
      "  Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /venv/main/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (2.19.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /venv/main/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: stack_data in /venv/main/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /venv/main/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (3.0.50)\n",
      "Requirement already satisfied: decorator in /venv/main/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /venv/main/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: exceptiongroup in /venv/main/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /venv/main/lib/python3.10/site-packages (from jupyter-core->jupyterlab) (4.3.6)\n",
      "Collecting jupyter-server-terminals>=0.4.4\n",
      "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Collecting nbconvert>=6.4.4\n",
      "  Downloading nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 KB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting argon2-cffi>=21.1\n",
      "  Downloading argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting overrides>=5.0\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting websocket-client>=1.7\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting terminado>=0.8.3\n",
      "  Downloading terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Collecting send2trash>=1.8.2\n",
      "  Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Collecting nbformat>=5.3.0\n",
      "  Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting prometheus-client>=0.9\n",
      "  Downloading prometheus_client-0.22.1-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.7/58.7 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jupyter-events>=0.11.0\n",
      "  Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Collecting babel>=2.10\n",
      "  Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m139.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting json5>=0.9.0\n",
      "  Downloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting jsonschema>=4.18.0\n",
      "  Downloading jsonschema-4.25.0-py3-none-any.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 KB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.3/198.3 KB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.9/222.9 KB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 KB\u001b[0m \u001b[31m297.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.1/326.1 KB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /venv/main/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.8/383.8 KB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Collecting rfc3339-validator\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting python-json-logger>=2.0.4\n",
      "  Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting rfc3986-validator>=0.1.1\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting bleach[css]!=5.0.0\n",
      "  Downloading bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.4/163.4 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.3/187.3 KB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nbclient>=0.5.0\n",
      "  Downloading nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Collecting mistune<4,>=2.0.3\n",
      "  Downloading mistune-3.1.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting defusedxml\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting fastjsonschema>=2.15\n",
      "  Downloading fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /venv/main/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /venv/main/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /venv/main/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.0)\n",
      "Requirement already satisfied: pure-eval in /venv/main/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /venv/main/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0\n",
      "  Downloading tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting webcolors>=24.6.0\n",
      "  Downloading webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Collecting fqdn\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting jsonpointer>1.13\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting isoduration\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting uri-template\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Collecting rfc3987-syntax>=1.1.0\n",
      "  Downloading rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
      "Collecting cffi>=1.0.1\n",
      "  Downloading cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.2/446.2 KB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting lark>=1.2.2\n",
      "  Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting arrow>=0.15.0\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting types-python-dateutil>=2.8.10\n",
      "  Downloading types_python_dateutil-2.9.0.20250708-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: webencodings, pytz, mpmath, fastjsonschema, xxhash, websocket-client, webcolors, uri-template, tzdata, types-python-dateutil, triton, tomli, tinycss2, threadpoolctl, terminado, sympy, soupsieve, sniffio, send2trash, scipy, safetensors, rpds-py, rfc3986-validator, rfc3339-validator, regex, python-json-logger, pycparser, pyarrow, propcache, prometheus-client, Pillow, pandocfilters, overrides, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, multidict, mistune, MarkupSafe, lark, jupyterlab-pygments, jsonpointer, json5, joblib, isodate, h11, frozenlist, fqdn, dill, defusedxml, bleach, babel, attrs, async-timeout, async-lru, aiohappyeyeballs, yarl, scikit-learn, rfc3987-syntax, referencing, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jupyter-server-terminals, jinja2, httpcore, cffi, beautifulsoup4, arrow, anyio, aiosignal, tokenizers, nvidia-cusolver-cu12, jsonschema-specifications, isoduration, httpx, argon2-cffi-bindings, aiohttp, transformers, torch, jsonschema, argon2-cffi, torchvision, torchaudio, sentence-transformers, nbformat, datasets, accelerate, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.3.0 accelerate-1.9.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 anyio-4.9.0 argon2-cffi-25.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 async-lru-2.0.5 async-timeout-5.0.1 attrs-25.3.0 babel-2.17.0 beautifulsoup4-4.13.4 bleach-6.2.0 cffi-1.17.1 datasets-4.0.0 defusedxml-0.7.1 dill-0.3.8 fastjsonschema-2.21.1 fqdn-1.5.1 frozenlist-1.7.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 isodate-0.7.2 isoduration-20.11.0 jinja2-3.1.6 joblib-1.5.1 json5-0.12.0 jsonpointer-3.0.0 jsonschema-4.25.0 jsonschema-specifications-2025.4.1 jupyter-events-0.12.0 jupyter-lsp-2.2.6 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.5 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 lark-1.2.2 mistune-3.1.3 mpmath-1.3.0 multidict-6.6.3 multiprocess-0.70.16 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 networkx-3.4.2 notebook-shim-0.2.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 overrides-7.7.0 pandas-2.3.1 pandocfilters-1.5.1 prometheus-client-0.22.1 propcache-0.3.2 pyarrow-21.0.0 pycparser-2.22 python-json-logger-3.3.0 pytz-2025.2 referencing-0.36.2 regex-2024.11.6 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-syntax-1.1.0 rpds-py-0.26.0 safetensors-0.5.3 scikit-learn-1.7.1 scipy-1.15.3 send2trash-1.8.3 sentence-transformers-3.3.1 sniffio-1.3.1 soupsieve-2.7 sympy-1.14.0 terminado-0.18.1 threadpoolctl-3.6.0 tinycss2-1.4.0 tokenizers-0.21.2 tomli-2.2.1 torch-2.3.0 torchaudio-2.3.0 torchvision-0.18.0 transformers-4.48.0 triton-2.3.0 types-python-dateutil-2.9.0.20250708 tzdata-2025.2 uri-template-1.3.0 webcolors-24.11.1 webencodings-0.5.1 websocket-client-1.8.0 xxhash-3.5.0 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# First, install compatible NumPy version\n",
    "%pip install numpy==1.26.4\n",
    "%pip install jupyterlab ipykernel ipywidgets requests isodate pandas datasets torch==2.3.0 torchvision torchaudio transformers==4.48.0 sentence-transformers==3.3.1 accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28c157-7c7b-4b9c-9b96-e09bb9f54b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY if you're on TPU (Kaggle/Colab TPU)\n",
    "!pip install torch==2.0.1 torchvision==0.15.2\n",
    "!pip install -U torch_xla==2.0 -f https://storage.googleapis.com/libtpu-releases/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174053fb-7724-47b1-b0b7-01c12e356b84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T06:52:30.428131Z",
     "iopub.status.busy": "2025-07-27T06:52:30.427868Z",
     "iopub.status.idle": "2025-07-27T06:53:00.392642Z",
     "shell.execute_reply": "2025-07-27T06:53:00.391717Z",
     "shell.execute_reply.started": "2025-07-27T06:52:30.428111Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akash-OMEN\\Desktop\\Web development files\\assignment-fluttr\\cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Add this import at the top\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.evaluation import TripletEvaluator, SentenceEvaluator\n",
    "\n",
    "from typing import List, Dict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe437b80-8367-43a1-b2b0-73b30069b201",
   "metadata": {},
   "source": [
    "### import model and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03b890",
   "metadata": {},
   "source": [
    "##  Load Pre-trained CLIP Model and Fashion Dataset\n",
    "\n",
    "Load the pre-trained CLIP ViT-L-14 model and prepare the fashion product images dataset for multi-modal embedding training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cdba8c1-0e18-4844-9349-e208dde5581b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T06:54:17.597114Z",
     "iopub.status.busy": "2025-07-27T06:54:17.596451Z",
     "iopub.status.idle": "2025-07-27T06:54:36.407450Z",
     "shell.execute_reply": "2025-07-27T06:54:36.406665Z",
     "shell.execute_reply.started": "2025-07-27T06:54:17.597089Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/clip-ViT-L-14\"\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786783c",
   "metadata": {},
   "source": [
    "# download the dataset from my public huggingface space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311d19ff-9553-49ef-bd8a-3e63456b5e93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T06:54:39.112809Z",
     "iopub.status.busy": "2025-07-27T06:54:39.112526Z",
     "iopub.status.idle": "2025-07-27T06:54:44.097457Z",
     "shell.execute_reply": "2025-07-27T06:54:44.096576Z",
     "shell.execute_reply.started": "2025-07-27T06:54:39.112787Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'styles', 'final_style'],\n",
      "        num_rows: 44419\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"dejasi5459/fashion-product-images-small\")\n",
    "\n",
    "# Explore the available splits\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b461de0-48ce-467b-aa3f-1660806349e4",
   "metadata": {},
   "source": [
    "### freeze model params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1affe3",
   "metadata": {},
   "source": [
    "## Configure Model Parameters for Fashion Fine-tuning\n",
    "\n",
    "Freeze most model parameters and only train specific layers (projection layer) to efficiently adapt CLIP for fashion product embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b8a3ab-fef2-4790-8df8-d554807a2ff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T06:54:50.295285Z",
     "iopub.status.busy": "2025-07-27T06:54:50.294736Z",
     "iopub.status.idle": "2025-07-27T06:54:50.300996Z",
     "shell.execute_reply": "2025-07-27T06:54:50.300468Z",
     "shell.execute_reply.started": "2025-07-27T06:54:50.295260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pick specific layers to train (note: you can add more layers to this list)\n",
    "trainable_layers_list = ['projection']\n",
    "\n",
    "# Apply freezing configuration\n",
    "for name, param in model.named_parameters():\n",
    "    # freeze all params\n",
    "    param.requires_grad = False\n",
    "    \n",
    "\n",
    "    # unfreeze layers in trainable_layers_list\n",
    "    if any(layer in name for layer in trainable_layers_list):\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8df4b51-3a26-408e-b0ec-2724462f7eb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T06:54:51.435374Z",
     "iopub.status.busy": "2025-07-27T06:54:51.435090Z",
     "iopub.status.idle": "2025-07-27T06:54:56.996974Z",
     "shell.execute_reply": "2025-07-27T06:54:56.996312Z",
     "shell.execute_reply.started": "2025-07-27T06:54:51.435353Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 0.model.visual_projection.weight\n",
      "Trainable: 0.model.text_projection.weight\n"
     ]
    }
   ],
   "source": [
    "# Verify trainable parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090b77c5-b965-472e-82b9-4f6c9fa9ee4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T06:55:00.970472Z",
     "iopub.status.busy": "2025-07-27T06:55:00.969635Z",
     "iopub.status.idle": "2025-07-27T06:55:00.981134Z",
     "shell.execute_reply": "2025-07-27T06:55:00.980382Z",
     "shell.execute_reply.started": "2025-07-27T06:55:00.970437Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 427,616,513\n",
      "Trainable parameters: 1,376,256\n",
      "Percentage of trainable parameters: 0.32%\n"
     ]
    }
   ],
   "source": [
    "# Count total and trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66aacde-b393-4a4d-8995-c864d3b56452",
   "metadata": {},
   "source": [
    "### preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547f566",
   "metadata": {},
   "source": [
    "##  Preprocess Fashion Product Dataset\n",
    "\n",
    "Load and preprocess the Fashion Product Images Dataset for multi-modal training. Create triplets of:\n",
    "- **Anchor**: Product images\n",
    "- **Positive**: Matching product descriptions (title + description)  \n",
    "- **Negative**: Non-matching product descriptions\n",
    "\n",
    "This enables contrastive learning to align similar fashion items in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "999f8992-dada-4030-ba2d-5820616c656b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T07:00:43.157607Z",
     "iopub.status.busy": "2025-07-27T07:00:43.157035Z",
     "iopub.status.idle": "2025-07-27T07:00:55.548071Z",
     "shell.execute_reply": "2025-07-27T07:00:55.546588Z",
     "shell.execute_reply.started": "2025-07-27T07:00:43.157576Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [03:27<00:00, 48.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define GPU-accelerated transforms\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224), interpolation=T.InterpolationMode.LANCZOS),  # Must be before ToTensor\n",
    "    T.ToTensor()\n",
    "])\n",
    "def preprocess_gpu(batch):\n",
    "    anchor_images = []\n",
    "    \n",
    "    for img in batch[\"image\"]:\n",
    "        try:\n",
    "            image = img.convert(\"RGB\")\n",
    "            # Convert to tensor and move to GPU\n",
    "            tensor = transform(image).to(device)\n",
    "            # Convert back to PIL if needed (or keep as tensor)\n",
    "            image = T.ToPILImage()(tensor.cpu())\n",
    "            anchor_images.append(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {e}\")\n",
    "            placeholder = Image.new('RGB', (224, 224), color='black')\n",
    "            anchor_images.append(placeholder)\n",
    "    \n",
    "    return {\n",
    "        \"anchor\": anchor_images,\n",
    "        \"positive\": [fs[\"Positive\"] for fs in batch[\"final_style\"]],\n",
    "        \"negative\": [fs[\"NEGATIVE\"] for fs in batch[\"final_style\"]],\n",
    "    }\n",
    "\n",
    "# Select first 10000 examples from the train split because of memory constraints, it can be increased later\n",
    "small_dataset = dataset[\"train\"].select(range(10000))\n",
    "columns_to_remove = [col for col in dataset['train'].column_names if col not in ['image', 'final_style']]\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_dataset = small_dataset.map(\n",
    "    preprocess_gpu,\n",
    "    batched=True,\n",
    "    batch_size=32,    # Efficient batch size for GPU\n",
    "    # num_proc=4,       # Parallel processing with multiple workers\n",
    "    remove_columns=columns_to_remove\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd98291c-cd08-499e-b7ef-7f401048495d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T07:04:06.442480Z",
     "iopub.status.busy": "2025-07-27T07:04:06.441990Z",
     "iopub.status.idle": "2025-07-27T07:04:06.456263Z",
     "shell.execute_reply": "2025-07-27T07:04:06.455572Z",
     "shell.execute_reply.started": "2025-07-27T07:04:06.442441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=60x80 at 0x27E0755A150>, 'final_style': {'Unnamed: 0': 4682, 'gender': 'Women', 'masterCategory': 'Apparel', 'subCategory': 'Topwear', 'articleType': 'Tops', 'baseColour': 'White', 'season': 'Summer', 'year': 2011.0, 'usage': 'Casual', 'productDisplayName': 'UCB Women Sleeveless White Top', 'Positive': 'Women , Apparel , Topwear , Tops , White , Summer , Casual , UCB Women Sleeveless White Top', 'NEGATIVE': 'Men , Lounge Pants , Winter , Formal'}, 'anchor': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=224x224 at 0x27E07559250>, 'positive': 'Women , Apparel , Topwear , Tops , White , Summer , Casual , UCB Women Sleeveless White Top', 'negative': 'Men , Lounge Pants , Winter , Formal'}\n"
     ]
    }
   ],
   "source": [
    "print(processed_dataset[0])  # for a single dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf77aaa-e001-40d5-ac53-b3e2b1bd77ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T07:05:00.063961Z",
     "iopub.status.busy": "2025-07-27T07:05:00.063656Z",
     "iopub.status.idle": "2025-07-27T07:05:00.072969Z",
     "shell.execute_reply": "2025-07-27T07:05:00.072256Z",
     "shell.execute_reply.started": "2025-07-27T07:05:00.063940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset=processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73ebb52d-bdbc-4df3-8be7-0378f1eebd1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T07:05:25.133407Z",
     "iopub.status.busy": "2025-07-27T07:05:25.132726Z",
     "iopub.status.idle": "2025-07-27T07:05:25.138244Z",
     "shell.execute_reply": "2025-07-27T07:05:25.137572Z",
     "shell.execute_reply.started": "2025-07-27T07:05:25.133384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'final_style', 'anchor', 'positive', 'negative'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac077fda-7c02-4055-91a2-bf505b080f7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T07:06:44.929200Z",
     "iopub.status.busy": "2025-07-27T07:06:44.928898Z",
     "iopub.status.idle": "2025-07-27T07:06:50.469988Z",
     "shell.execute_reply": "2025-07-27T07:06:50.469099Z",
     "shell.execute_reply.started": "2025-07-27T07:06:44.929178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8000/8000 [02:21<00:00, 56.62 examples/s] \n",
      "Map: 100%|██████████| 1000/1000 [00:17<00:00, 57.73 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:17<00:00, 56.01 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['anchor', 'positive', 'negative'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['anchor', 'positive', 'negative'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['anchor', 'positive', 'negative'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# STEP 1: Split into train/valid/test\n",
    "train_valid = processed_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "valid_test = train_valid[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_valid[\"train\"],\n",
    "    \"valid\": valid_test[\"train\"],\n",
    "    \"test\":  valid_test[\"test\"]\n",
    "})\n",
    "\n",
    "# STEP 2: Keep only necessary columns\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    dataset[split] = dataset[split].select_columns(['anchor', 'positive', 'negative'])\n",
    "\n",
    "# STEP 3: Ensure 'positive' and 'negative' are strings (for text encoders)\n",
    "def ensure_text(example):\n",
    "    return {\n",
    "        \"anchor\": example[\"anchor\"],  # Usually a PIL.Image or tensor\n",
    "        \"positive\": str(example[\"positive\"]),\n",
    "        \"negative\": str(example[\"negative\"])\n",
    "    }\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    dataset[split] = dataset[split].map(ensure_text)\n",
    "\n",
    "#  Final confirmation\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1919ac-db6c-417e-8912-db8deff0f758",
   "metadata": {},
   "source": [
    "### eval pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669dea96",
   "metadata": {},
   "source": [
    "## Evaluate Pre-trained Model on Fashion Data\n",
    "\n",
    "Test the baseline performance of the pre-trained CLIP model on fashion product data before fine-tuning. This establishes our baseline metrics for fashion product retrieval performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "621a134e-f623-426c-9edf-3305af59c70e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T07:07:47.826854Z",
     "iopub.status.busy": "2025-07-27T07:07:47.826113Z",
     "iopub.status.idle": "2025-07-27T07:07:47.831360Z",
     "shell.execute_reply": "2025-07-27T07:07:47.830451Z",
     "shell.execute_reply.started": "2025-07-27T07:07:47.826828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Updated create_triplet_evaluator with smaller batch size to avoid memory issues\n",
    "def create_triplet_evaluator(set_name):\n",
    "    \"\"\"\n",
    "    Create triplet evaluator for \"train\", \"valid\", or \"test\" split\n",
    "    \"\"\"\n",
    "    data = dataset[set_name]\n",
    "    # Take smaller subset for evaluation to avoid memory issues\n",
    "    max_samples = min(100, len(data[\"anchor\"]))  # Limit to 100 samples for evaluation\n",
    "    \n",
    "    anchors = list(data[\"anchor\"][:max_samples])\n",
    "    positives = list(data[\"positive\"][:max_samples])\n",
    "    negatives = list(data[\"negative\"][:max_samples])\n",
    "    \n",
    "    return TripletEvaluator(\n",
    "        anchors=anchors,\n",
    "        positives=positives,\n",
    "        negatives=negatives,\n",
    "        name=f\"fashion-{set_name}\",\n",
    "        batch_size=4,  # Use smaller batch size\n",
    "        show_progress_bar=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eb7ec04-890e-43ae-902f-d83c0c16cd16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T07:07:51.326918Z",
     "iopub.status.busy": "2025-07-27T07:07:51.326169Z",
     "iopub.status.idle": "2025-07-27T07:07:59.960886Z",
     "shell.execute_reply": "2025-07-27T07:07:59.960200Z",
     "shell.execute_reply.started": "2025-07-27T07:07:51.326890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 25/25 [00:05<00:00,  4.46it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 47.18it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 63.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'fashion-train_cosine_accuracy': 0.9700000286102295}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 25/25 [00:04<00:00,  5.50it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 55.71it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 58.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: {'fashion-valid_cosine_accuracy': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator_train = create_triplet_evaluator(\"train\")\n",
    "evaluator_valid = create_triplet_evaluator(\"valid\")\n",
    "print(\"Train:\", evaluator_train(model))\n",
    "print(\"Valid:\", evaluator_valid(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb73c7b7-9b82-46d2-930c-29a7058074e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T07:08:07.679994Z",
     "iopub.status.busy": "2025-07-27T07:08:07.679393Z",
     "iopub.status.idle": "2025-07-27T07:08:07.687411Z",
     "shell.execute_reply": "2025-07-27T07:08:07.686634Z",
     "shell.execute_reply.started": "2025-07-27T07:08:07.679972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageTextRetrievalEvaluator(SentenceEvaluator):\n",
    "    \"\"\"\n",
    "    \n",
    "    Custom evaluator for fashion product image-text retrieval performance \n",
    "    Measures Recall k: how often the correct product description is found in the top-k most similar items \n",
    "    for each fashion product image\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        images: List,\n",
    "        texts: List[str],\n",
    "        name: str = '',\n",
    "        k: int = 1,\n",
    "        batch_size: int = 32,\n",
    "        show_progress_bar: bool = False\n",
    "    ):\n",
    "        # Limit dataset size for evaluation to avoid memory issues\n",
    "        max_samples = min(100, len(images))\n",
    "        self.images = list(images[:max_samples])\n",
    "        self.texts = list(texts[:max_samples])\n",
    "        self.name = name\n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "        self.show_progress_bar = show_progress_bar\n",
    "\n",
    "    def __call__(self,\n",
    "        model: SentenceTransformer,\n",
    "        output_path: str = None,\n",
    "        epoch: int = -1,\n",
    "        steps: int = -1) -> Dict[str, float]:\n",
    "        \n",
    "        # Get embeddings for all images\n",
    "        img_embeddings = model.encode(\n",
    "            self.images,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=self.show_progress_bar,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        \n",
    "        # Get embeddings for all texts\n",
    "        text_embeddings = model.encode(\n",
    "            self.texts,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=self.show_progress_bar,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        cos_scores = torch.nn.functional.cosine_similarity(\n",
    "            img_embeddings.unsqueeze(1),\n",
    "            text_embeddings.unsqueeze(0),\n",
    "            dim=2\n",
    "        )\n",
    "        \n",
    "        # Get indices of top k predictions for each image\n",
    "        _, top_indices = torch.topk(cos_scores, k=self.k, dim=1)\n",
    "        \n",
    "        # Calculate Recall@k (correct if ground truth index is in top k predictions)\n",
    "        correct = sum(i in top_indices[i].tolist() for i in range(len(self.images)))\n",
    "        recall_at_k = correct / len(self.images)\n",
    "\n",
    "        return {f'{self.name}_Recall@{self.k}': recall_at_k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00119a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae771da5-0b88-433c-9694-5423dca7a058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T07:08:30.540295Z",
     "iopub.status.busy": "2025-07-27T07:08:30.540022Z",
     "iopub.status.idle": "2025-07-27T07:08:30.544641Z",
     "shell.execute_reply": "2025-07-27T07:08:30.543933Z",
     "shell.execute_reply.started": "2025-07-27T07:08:30.540277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_recall_evaluator(set_name, k=1):\n",
    "    \"\"\"\n",
    "        Create recall evaluator for \"train\", \"valid\", or \"test\" split\n",
    "    \"\"\"\n",
    "    # Convert to lists to avoid numpy indexing issues\n",
    "    data = dataset[set_name]\n",
    "    \n",
    "    return ImageTextRetrievalEvaluator(\n",
    "        images=list(data[\"anchor\"]),\n",
    "        texts=list(data[\"positive\"]),\n",
    "        name=f\"faahion-recall-{set_name}\",\n",
    "        k=k,\n",
    "        batch_size=4  # Smaller batch size to avoid memory issues\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f67f59f9-3a23-4e4a-8def-1279431d93c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T07:08:33.909829Z",
     "iopub.status.busy": "2025-07-27T07:08:33.909136Z",
     "iopub.status.idle": "2025-07-27T07:08:40.931384Z",
     "shell.execute_reply": "2025-07-27T07:08:40.930730Z",
     "shell.execute_reply.started": "2025-07-27T07:08:33.909807Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'faahion-recall-train_Recall@1': 0.57}\n",
      "Valid: {'faahion-recall-valid_Recall@1': 0.65}\n"
     ]
    }
   ],
   "source": [
    "# Create new evaluator with Recall@k\n",
    "evaluator_recall_train = create_recall_evaluator(\"train\", k=1)\n",
    "evaluator_recall_valid = create_recall_evaluator(\"valid\", k=1)\n",
    "\n",
    "print(\"Train:\", evaluator_recall_train(model))\n",
    "print(\"Valid:\", evaluator_recall_valid(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534a4f7-524a-466a-9995-0df5eb72ffc4",
   "metadata": {},
   "source": [
    "### define training args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e54aeea-f468-47c7-baa9-abf61ef0ebc9",
   "metadata": {},
   "source": [
    "### fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00af3613-8934-41fb-ad85-cbc97b54c3eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T07:14:08.749517Z",
     "iopub.status.busy": "2025-07-27T07:14:08.748797Z",
     "iopub.status.idle": "2025-07-27T07:14:08.777386Z",
     "shell.execute_reply": "2025-07-27T07:14:08.776813Z",
     "shell.execute_reply.started": "2025-07-27T07:14:08.749496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define loss (note: loss expects columns to be ordered as anchor-positive-negative)\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 2\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "finetuned_model_name = \"clip-fashion-embeddings-10k-ft\"\n",
    "\n",
    "train_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=f\"models/{finetuned_model_name}\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    # Evaluation settings\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "    logging_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9363a301-9c6c-4478-bc46-1ffb8493f2c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T07:14:31.996707Z",
     "iopub.status.busy": "2025-07-27T07:14:31.996432Z",
     "iopub.status.idle": "2025-07-27T07:15:06.264161Z",
     "shell.execute_reply": "2025-07-27T07:15:06.263094Z",
     "shell.execute_reply.started": "2025-07-27T07:14:31.996690Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 16:16, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Faahion-recall-train Recall@1</th>\n",
       "      <th>Faahion-recall-valid Recall@1</th>\n",
       "      <th>Sequential Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.120200</td>\n",
       "      <td>0.158124</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.127873</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 29min 58s\n",
      "Wall time: 16min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.184337726441212, metrics={'train_runtime': 978.381, 'train_samples_per_second': 16.354, 'train_steps_per_second': 1.022, 'total_flos': 0.0, 'train_loss': 0.184337726441212, 'epoch': 2.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"valid\"],\n",
    "    loss=loss,\n",
    "    evaluator=[evaluator_recall_train, evaluator_recall_valid],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c263c0ef-e40e-4e1c-b29a-53847509fa42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T07:17:17.743259Z",
     "iopub.status.busy": "2025-07-27T07:17:17.742953Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training will run on: cuda\n",
      "test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 26:40, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Faahion-recall-train Recall@1</th>\n",
       "      <th>Faahion-recall-valid Recall@1</th>\n",
       "      <th>Sequential Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>0.248443</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.245227</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>0.240613</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>0.239504</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.237289</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.238423</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.232531</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.234200</td>\n",
       "      <td>0.232669</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>0.231574</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.231298</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.1683788457810879, metrics={'train_runtime': 1601.6873, 'train_samples_per_second': 49.947, 'train_steps_per_second': 1.561, 'total_flos': 0.0, 'train_loss': 0.1683788457810879, 'epoch': 10.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "\n",
    "from sentence_transformers.evaluation import TripletEvaluator  # or any evaluator you're using\n",
    "\n",
    "import torch\n",
    "\n",
    "# Confirm GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training will run on: {device}\")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "#  Define loss\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "#  Hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "lr = 1e-5\n",
    "finetuned_model_name = \"clip-fashionAssign-embeddings\"\n",
    "\n",
    "# Training arguments\n",
    "train_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=f\"models/{finetuned_model_name}\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=1,\n",
    "    save_total_limit=2,\n",
    "    fp16=True  # Optional: Enable mixed precision (faster on modern GPUs)\n",
    ")\n",
    "\n",
    "# Training\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"valid\"],\n",
    "    loss=loss,\n",
    "    evaluator=[evaluator_recall_train, evaluator_recall_valid],\n",
    ")\n",
    "print(\"test\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac741a-69f9-4de4-a816-dba229fbb5b0",
   "metadata": {},
   "source": [
    "### evaluate fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5631ffa9",
   "metadata": {},
   "source": [
    "### Load Epoch 4 Model for Evaluation\n",
    "\n",
    "Load the model from epoch 4 (checkpoint-1000) from the clip-fashion-embeddings-10k-ft training for evaluation and hf upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b228b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded epoch 4 model from: models/clip-fashion-embeddings-10k-ft/checkpoint-1000\n"
     ]
    }
   ],
   "source": [
    "# Load the epoch 4 model from checkpoint-1000 (clip-fashion-embeddings-10k-ft)\n",
    "epoch_4_model_path = \"models/clip-fashion-embeddings-10k-ft/checkpoint-1000\"\n",
    "epoch_4_model = SentenceTransformer(epoch_4_model_path)\n",
    "\n",
    "print(f\"Loaded epoch 4 model from: {epoch_4_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a957fc9e-0da5-4c71-8ac4-c5f330c3d817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation with Epoch 4 Model (checkpoint-1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 25/25 [00:05<00:00,  4.96it/s]\n",
      "Batches:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 50.88it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 50.88it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 59.95it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'fashion-train_cosine_accuracy': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 25/25 [00:04<00:00,  5.42it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:04<00:00,  5.42it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 60.68it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 60.68it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 63.78it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 63.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: {'fashion-valid_cosine_accuracy': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 25/25 [00:04<00:00,  5.38it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:04<00:00,  5.38it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 62.54it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 62.54it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 64.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: {'fashion-test_cosine_accuracy': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator_test = create_triplet_evaluator(\"test\")\n",
    "\n",
    "# Evaluate using the epoch 4 model (checkpoint-1000)\n",
    "print(\" Evaluation with Epoch 4 Model (checkpoint-1000)\")\n",
    "print(\"Train:\", evaluator_train(epoch_4_model))\n",
    "print(\"Valid:\", evaluator_valid(epoch_4_model))\n",
    "print(\"Test:\", evaluator_test(epoch_4_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dae887ae-3f18-4456-b0ed-4b5587cbc461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Evaluation with Epoch 4 Model (checkpoint-1000)\n",
      "Train: {'faahion-recall-train_Recall@1': 0.75}\n",
      "Train: {'faahion-recall-train_Recall@1': 0.75}\n",
      "Valid: {'faahion-recall-valid_Recall@1': 0.75}\n",
      "Valid: {'faahion-recall-valid_Recall@1': 0.75}\n",
      "Test: {'faahion-recall-test_Recall@1': 0.78}\n",
      "Test: {'faahion-recall-test_Recall@1': 0.78}\n"
     ]
    }
   ],
   "source": [
    "evaluator_recall_test = create_recall_evaluator(\"test\")\n",
    "\n",
    "# Evaluate using the epoch 4 model (checkpoint-1000)\n",
    "print(\"Recall Evaluation with Epoch 4 Model (checkpoint-1000)\")\n",
    "print(\"Train:\", evaluator_recall_train(epoch_4_model))\n",
    "print(\"Valid:\", evaluator_recall_valid(epoch_4_model))\n",
    "print(\"Test:\", evaluator_recall_test(epoch_4_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "421b5910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 1.71G/1.71G [02:59<00:00, 9.50MB/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pushed dejasi5459/clip-fashion-embeddings-final-10k-ft\n"
     ]
    }
   ],
   "source": [
    "# Push the epoch 4 model to Hugging Face Hub\n",
    "epoch_4_model_name = \"clip-fashion-embeddings-final-10k-ft\"\n",
    "\n",
    "try:\n",
    "    epoch_4_model.push_to_hub(f\"dejasi5459/{epoch_4_model_name}\")\n",
    "    print(f\"Successfully pushed dejasi5459/{epoch_4_model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error pushing model to hub: {e}\")\n",
    "    print(\"Make sure you're logged in to Hugging Face\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
